\section{Asynchronous Methods for Deep Reinforcement Learning}
%By \cite{Mnih2016AsynchronousLearning}.
In \cite{Mnih2016AsynchronousLearning} experience replay isn't used because of memory and computation usage and because it is off-policy, which means that it learns from data generated by a policy that may already be outdated. Instead, we use multiple agents (that use well-known reinforcement algorithms) that run in parallel but each running on a separate instance of the same type of environment. These all run on a multi-core CPU (i.e. on only one computer). Each of the agents use the same parameters $\theta$, but by using certain exploration policies, they are able to each explore a possibly different part of the state space. It is possible to use a different exploration policy for each agent as to maximize the exploration of different states. Each agent also calculates a gradient w.r.t. the parameters $\theta$ depending on its experience. These gradients are accumulated and after a certain number of steps they are applied to the parameters $\theta$. Because they are global, this update has an effect on all agents. After an amount (possibly the same amount as for the gradient update) of steps, the target network parameters $\theta^{-}$ can also be updated using the current parameters $\theta$.\\
In $n$-step Q-learning and advantage actor-critic, a forward-view is used instead of the more commonly used backward view (which uses eligibility traces). Forward view was found to be easier when training neural networks with momentum-based methods and backpropagation through time. To do this, we first compute a certain number of steps (or until the episode is over). Then, the gradients are computed for each state-action pair encountered since the last update. Each n-step update uses the longest possible n-step return resulting in a one-step update for the last state, a two-step update for the second last state, and so on for a total of up to the previously determined number of maximum allowed steps. These accumulated gradients are then immediately applied to $\theta$. This results in the pseudo-code shown in Algorithm~\ref{algo:a3c}.\\
\begin{algorithm}[htb]
\DontPrintSemicolon
\emph{// Assume global shared parameter vectors $\theta$ and $\theta_v$ and global shared counter $T=0$}\;
\emph{// Assume thread-specific parameter vectors $\theta'$ and $\theta'_v$}\;
Initialize thread step counter $t\gets 1$\;
\Repeat{$T > T_{max}$}{
    Reset gradients: $d\theta \gets 0$ and $d\theta_v \gets 0$\;
    Synchronize agent-specific parameters  $\theta'=\theta$ and $\theta'_v=\theta_v$\;
    $t_{start} \gets t$\;
    Initialize state $s_t$\;
    \Repeat{terminal $s_t$ \textbf{or} $t-t_{start}==t_{max}$}{
        Perform $a_t$ according to policy $\pi (a_t|s_t;\theta')$\;
        Receive reward $r_t$ and new state $s_{t+1}$\;
        $t \gets t + 1$\;
        $T \gets T + 1$\;
        }
        $R =
    \left\{
    \begin{array}{l l}
      0  \quad & \text{if $s_t$ is terminal}\\
        V(s_t,\theta'_v) \quad & \text{otherwise // Bootstrap from last state}
    \end{array}\right.$
    \For {$i \in \{t-1,\ldots,t_{start} \}$} {
        Accumulate gradients w.r.t. $\theta'$: $d\theta \gets d\theta + \nabla_{\theta'} \log\pi(a_i|s_i;\theta') (R - V(s_i;\theta'_v))$\;
        Accumulate gradients w.r.t. $\theta'_v$: $d\theta_v \gets d\theta_v + {\partial\left(R - V(s_i;\theta'_v)\right)^2}/{\partial \theta'_v}$\;
    }
    Perform asynchronous update of $\theta$ using $d\theta$ and of $\theta_v$ using $d\theta_v$\;
}
\caption[Asynchronous Advantage Actor Critic]{Asynchronous Advantage Actor Critic (A3C). Source: \cite{Mnih2016AsynchronousLearning}.}
\label{algo:a3c}
\end{algorithm}

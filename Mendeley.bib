@inproceedings{choromanska2015loss,
    title = {{The Loss Surfaces of Multilayer Networks.}},
    year = {2015},
    booktitle = {AISTATS},
    author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann}
}

@article{bengio1994learning,
    title = {{Learning long-term dependencies with gradient descent is difficult}},
    year = {1994},
    journal = {IEEE transactions on neural networks},
    author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
    number = {2},
    pages = {157--166},
    volume = {5},
    publisher = {IEEE}
}

@article{matsugu2003subject,
    title = {{Subject independent facial expression recognition with robust face detection using a convolutional neural network}},
    year = {2003},
    journal = {Neural Networks},
    author = {Matsugu, Masakazu and Mori, Katsuhiko and Mitari, Yusuke and Kaneda, Yuji},
    number = {5},
    pages = {555--559},
    volume = {16},
    publisher = {Elsevier}
}

@book{ML,
    title = {{Machine Learning}},
    year = {1997},
    author = {Mitchell, Tom},
    publisher = {McGraw Hill},
    keywords = {learning machine mitchell tom}
}

@misc{mnih2013playing,
    title = {{Playing Atari with Deep Reinforcement Learning}},
    year = {2013},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
    url = {http://arxiv.org/abs/1312.5602},
    keywords = {arxiv cs}
}

@misc{1606.01540,
    title = {{OpenAI Gym}},
    year = {2016},
    author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech}
}

@inproceedings{conf/ijcai/SelfridgeSB85,
    title = {{Training and Tracking in Robotics.}},
    year = {1985},
    booktitle = {IJCAI},
    author = {Selfridge, Oliver G and Sutton, Richard S and Barto, Andrew G},
    editor = {Joshi, Aravind K},
    pages = {670--672},
    publisher = {Morgan Kaufmann},
    url = {http://dblp.uni-trier.de/db/conf/ijcai/ijcai85.html#SelfridgeSB85},
    keywords = {dblp}
}

@inproceedings{lazaric2008transfer,
    title = {{Transfer of samples in batch reinforcement learning}},
    year = {2008},
    booktitle = {Proceedings of the 25th international conference on Machine learning},
    author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
    pages = {544--551},
    organization = {ACM}
}

@techreport{Bernstein99reusingold,
    title = {{Reusing Old Policies to Accelerate Learning on New MDPs}},
    year = {1999},
    author = {Bernstein, Daniel S}
}

@article{perkins1999using,
    title = {{Using options for knowledge transfer in reinforcement learning}},
    year = {1999},
    journal = {University of Massachusetts, Amherst, MA, USA, Tech. Rep},
    author = {Perkins, Theodore J and Precup, Doina and {others}}
}

@article{journals/ml/FosterD02,
    title = {{Structure in the Space of Value Functions.}},
    year = {2002},
    journal = {Machine Learning},
    author = {Foster, David J and Dayan, Peter},
    number = {2-3},
    pages = {325--346},
    volume = {49},
    url = {http://dblp.uni-trier.de/db/journals/ml/ml49.html#FosterD02},
    keywords = {dblp}
}

@article{MnihDqn,
    title = {{Dqn}},
    author = {Mnih, Volodymyr and Silver, David and Riedmiller, Martin},
    pages = {1--9},
    isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
    doi = {10.1038/nature14236},
    issn = {0028-0836},
    pmid = {25719670},
    arxivId = {1312.5602}
}

@article{williams1992simple,
  added-at = {2009-12-05T22:23:20.000+0100},
  author = {Williams, R.J.},
  biburl = {https://www.bibsonomy.org/bibtex/22dcb027420b1770b0ab6cba582eeee99/monodie},
  interhash = {b90d65a735ae02a940f5075b0fd7ebe7},
  intrahash = {2dcb027420b1770b0ab6cba582eeee99},
  journal = {Machine Learning},
  keywords = {imported},
  number = 3,
  pages = {229--256},
  publisher = {Springer},
  timestamp = {2009-12-07T08:56:12.000+0100},
  title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
  volume = 8,
  year = 1992
}

@article{Szepesvari2010AlgorithmsLearning,
    title = {{Algorithms for Reinforcement Learning}},
    year = {2010},
    journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
    author = {Szepesv{\'{a}}ri, Csaba},
    number = {1},
    pages = {1--103},
    volume = {4},
    isbn = {9781608454921},
    doi = {10.2200/S00268ED1V01Y201005AIM009},
    issn = {1939-4608}
}

@article{Brockman2016OpenAIGym,
    title = {{OpenAI Gym}},
    year = {2016},
    journal = {arXiv},
    author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
    pages = {1--4},
    url = {http://arxiv.org/abs/1606.01540},
    arxivId = {1606.01540}
}

@article{Mnih2015Human-levelLearning,
    title = {{Human-level control through deep reinforcement learning}},
    year = {2015},
    journal = {Nature},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    number = {7540},
    pages = {529--533},
    volume = {518},
    publisher = {Nature Publishing Group},
    url = {http://dx.doi.org/10.1038/nature14236},
    isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
    doi = {10.1038/nature14236},
    issn = {0028-0836},
    pmid = {25719670},
    arxivId = {1312.5602}
}

@article{Lillicrap2015ContinuousLearning,
    title = {{Continuous control with deep reinforcement learning}},
    year = {2015},
    journal = {arXiv preprint arXiv:1509.02971},
    author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
    pages = {1--14},
    url = {http://arxiv.org/abs/1509.02971},
    isbn = {2200000006},
    doi = {10.1561/2200000006},
    issn = {1935-8237},
    pmid = {24966830},
    arxivId = {1509.02971}
}

@article{Mnih2016AsynchronousLearning,
    title = {{Asynchronous Methods for Deep Reinforcement Learning}},
    year = {2016},
    journal = {arXiv},
    author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
    pages = {1--28},
    volume = {48},
    url = {http://arxiv.org/abs/1602.01783},
    pmid = {1000272564},
    arxivId = {1602.01783}
}

@article{Tieleman2012LectureMagnitude.,
    title = {{Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.}},
    year = {2012},
    journal = {COURSERA: Neural Networks for Machine Learning},
    author = {Tieleman, Tijmen and Hinton, Geoffrey}
}

@article{Isele2016UsingLearning,
    title = {{Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning}},
    year = {2016},
    journal = {Ijcai},
    author = {Isele, David and Eaton, Eric},
    pages = {1620--1626},
    pmid = {91246},
    keywords = {Machine Learning}
}

@article{Yang2010ImageRepresentation,
    title = {{Image Super-Resolution Via Sparse Representation}},
    year = {2010},
    journal = {IEEE Transactions on Image Processing},
    author = {Yang, Jianchao Yang Jianchao and Wright, J and Huang, T S and Ma, Yi Ma Yi},
    pages = {1349--1352},
    volume = {19},
    url = {http://www.ncbi.nlm.nih.gov/pubmed/20483687},
    isbn = {9781457713026},
    doi = {10.1109/TIP.2010.2050625},
    issn = {10577149},
    pmid = {20483687},
    keywords = {self learning, sparse representation, super resolution, support vector regression}
}

@article{Sutton1999PolicyApproximation,
    title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
    year = {1999},
    journal = {In Advances in Neural Information Processing Systems 12},
    author = {Sutton, Richard S. and Mcallester, David and Singh, Satinder and Mansour, Yishay},
    pages = {1057--1063},
    isbn = {0-262-19450-3},
    doi = {10.1.1.37.9714},
    issn = {0047-2875}
}

@article{Cho2014LearningTranslation,
    title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
    year = {2014},
    journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
    pages = {1724--1734},
    url = {http://arxiv.org/abs/1406.1078},
    isbn = {9781937284961},
    doi = {10.3115/v1/D14-1179},
    issn = {09205691},
    pmid = {2079951},
    arxivId = {1406.1078},
    keywords = {decoder, for statistical machine translation, rning phrase representations using, rnn encoder}
}

@article{Wu2009FunctionSystems,
    title = {{Function Approximation Using Tile and Kanerva Coding For Multi-Agent Systems}},
    year = {2009},
    journal = {Proc. Of Adaptive Learning Agents Workshop (ALA {\ldots}},
    author = {Wu, Cheng and Meleis, Waleed},
    number = {Aamas},
    url = {http://www.cs.lafayette.edu/~taylorm/ALA09/4.pdf},
    keywords = {function approximation, fuzzy logic, reinforcement learning}
}

@article{Salimans2017EvolutionLearning,
    title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
    year = {2017},
    author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
    url = {http://arxiv.org/abs/1703.03864},
    arxivId = {1703.03864}
}

@article{Wang2016LearningLearn,
    title = {{Learning to reinforcement learn}},
    year = {2016},
    journal = {arXiv},
    author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
    pages = {1--17},
    url = {http://arxiv.org/abs/1611.05763},
    arxivId = {1611.05763}
}

@article{Kirkpatrick2016OvercomingNetworks,
    title = {{Overcoming catastrophic forgetting in neural networks}},
    year = {2016},
    journal = {arXiv preprint},
    author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
    doi = {10.1073/PNAS.1611835114},
    issn = {0027-8424},
    arxivId = {1612.00796}
}

@article{Amos2017OptNet:Networks,
    title = {{OptNet: Differentiable Optimization as a Layer in Neural Networks}},
    year = {2017},
    author = {Amos, Brandon and Kolter, J. Zico},
    number = {Icml},
    arxivId = {1703.00443}
}

@article{Schulman2015TrustOptimization,
    title = {{Trust Region Policy Optimization}},
    year = {2015},
    journal = {Icml-2015},
    author = {Schulman, John and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
    pages = {16},
    url = {http://arxiv.org/abs/1502.0547},
    isbn = {0375-9687},
    doi = {10.1063/1.4927398},
    issn = {2158-3226},
    arxivId = {1502.0547}
}

@article{Nachum2016BridgingLearning,
    title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
    year = {2016},
    author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Brain, Google},
    number = {4},
    arxivId = {1702.08892}
}

@article{Fernando2017PathNet:Networks,
    title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
    year = {2017},
    author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
    arxivId = {1701.08734},
    keywords = {basal ganglia, continual learning, evolution and, giant networks, learning, multitask, path evolution algorithm, transfer learning}
}

@book{Sutton1998ReinforcementIntroduction,
    title = {{Reinforcement Learning: An Introduction}},
    year = {1998},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    publisher = {MIT Press},
    address = {Cambridge}
}

@article{Srivastava2014Dropout:Overfitting,
    title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
    year = {2014},
    journal = {Journal of Machine Learning Research},
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    pages = {1929--1958},
    volume = {15},
    isbn = {1532-4435},
    doi = {10.1214/12-AOS1000},
    issn = {15337928},
    arxivId = {1102.4807},
    keywords = {deep learning, model combination, neural networks, regularization}
}

@article{Sigaud2016DeepAlgorithms,
    title = {{Deep Reinforcement Learning Algorithms}},
    year = {2016},
    author = {Sigaud, Olivier},
    number = {May}
}

@article{LeCun2015DeepLearning,
    title = {{Deep learning}},
    year = {2015},
    journal = {Nature},
    author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    number = {7553},
    pages = {436--444},
    volume = {521},
    url = {http://dx.doi.org/10.1038/nature14539},
    isbn = {9780521835688},
    doi = {10.1038/nature14539},
    issn = {0028-0836},
    pmid = {10463930},
    arxivId = {arXiv:1312.6184v5}
}

@article{Lange2012AutonomousApplication,
    title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
    year = {2012},
    journal = {Building Technique Development},
    author = {Lange, Stanislav and Riedmiller, Martin and Voigtlander, A},
    pages = {1--8},
    volume = {20}
}

@article{Peters2008NaturalActor-critic,
    title = {{Natural actor-critic}},
    year = {2008},
    journal = {Neurocomputing},
    author = {Peters, Jan and Schaal, Stefan},
    number = {7},
    pages = {1180--1190},
    volume = {71}
}

@article{Schulman2016TheResearch,
    title = {{The Nuts and Bolts of Deep RL Research}},
    year = {2016},
    author = {Schulman, John}
}

@article{SilverLectureGradient,
    title = {{Lecture 7 : Policy Gradient}},
    journal = {Policy},
    author = {Silver, David}
}

@article{Taylor2009TransferSurvey,
    title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
    year = {2009},
    journal = {Journal of Machine Learning Research},
    author = {Taylor, Matthew E and Stone, Peter},
    pages = {1633--1685},
    volume = {10},
    url = {http://portal.acm.org/citation.cfm?id=1755839},
    isbn = {15324435},
    doi = {10.1007/978-3-642-27645-3},
    issn = {15324435},
    pmid = {260529900010},
    arxivId = {0803.0476},
    keywords = {1, 1998, actions with goal, example, leaning agents take sequential, maximizing a reward, multi task learning, problems, reinforcement learning, rl, signal, sutton barto, transfer learning, transfer learning objectives, which may time delayed}
}

@article{Lazaric2012TransferSurvey,
    title = {{Transfer in Reinforcement Learning: a Framework and a Survey}},
    year = {2012},
    journal = {Reinforcement Learning: State of the Art},
    author = {Lazaric, Alessandro},
    pages = {143--173},
    volume = {12},
    url = {http://link.springer.com/10.1007/978-3-642-27645-3},
    isbn = {978-3-642-27644-6},
    doi = {10.1007/978-3-642-27645-3},
    issn = {18726240}
}

@inproceedings{fernandez2006probabilistic,
  author={Fern{\'a}ndez, Fernando and Veloso, Manuela},
  biburl = {https://www.bibsonomy.org/bibtex/2b2acaf4238a58512ce3bc8d6ec5151aa/dblp},
  booktitle = {AAMAS},
  crossref = {conf/atal/2006},
  date = {2006-09-27},
  description = {dblp},
  editor = {Nakashima, Hideyuki and Wellman, Michael P. and Weiss, Gerhard and Stone, Peter},
  ee = {http://doi.acm.org/10.1145/1160633.1160762},
  interhash = {6d01f43fa9b3c00ba69c8384e144087b},
  intrahash = {b2acaf4238a58512ce3bc8d6ec5151aa},
  isbn = {1-59593-303-4},
  keywords = {dblp},
  pages = {720-727},
  publisher = {ACM},
  timestamp = {2006-09-27T00:00:00.000+0200},
  title = {Probabilistic policy reuse in a reinforcement learning agent.},
  url = {http://dblp.uni-trier.de/db/conf/atal/aamas2006.html#FernandezV06},
  year = 2006
}

@inproceedings{walsh2006transferring,
  title={Transferring state abstractions between MDPs},
  author={Walsh, Thomas J and Li, Lihong and Littman, Michael L},
  booktitle={ICML Workshop on Structural Knowledge Transfer for Machine Learning},
  year={2006}
}

@inproceedings{conf/cira/TanakaY03,
  added-at = {2014-03-12T00:00:00.000+0100},
  author = {Tanaka, Fumihide and Yamamura, Masayuki},
  biburl = {https://www.bibsonomy.org/bibtex/26d1c9e8ee19ccd8ec1651b7b24b081be/dblp},
  booktitle = {CIRA},
  crossref = {conf/cira/2003},
  ee = {http://dx.doi.org/10.1109/CIRA.2003.1222152},
  interhash = {328c1a8fe0cd11445ae487df84a35429},
  intrahash = {6d1c9e8ee19ccd8ec1651b7b24b081be},
  isbn = {0-7803-7866-0},
  keywords = {dblp},
  pages = {1108-1113},
  publisher = {IEEE},
  timestamp = {2015-06-18T17:47:25.000+0200},
  title = {Multitask reinforcement learning on the distribution of MDPs.},
  url = {http://dblp.uni-trier.de/db/conf/cira/cira2003.html#TanakaY03},
  year = 2003
}

@phdthesis{lazaric2008knowledge,
    Author = {Alessandro Lazaric},
    Bib2Html_Pubtype = {Ph.D. Dissertation},
    Bib2Html_Rescat = {Transfer Learning},
    Date-Modified = {2012-01-31 13:13:37 +0100},
    Owner = {leto},
    School = {Politecnico di Milano},
    Timestamp = {2010.02.09},
    Title = {Knowledge Transfer in Reinforcement Learning},
    Year = {2008}}

@inproceedings{sunmola2006model,
  title={Model transfer for Markov decision tasks via parameter matching},
  author={Sunmola, Funlade T and Wyatt, Jeremy L},
  booktitle={Proceedings of the 25th Workshop of the UK Planning and Scheduling Special Interest Group (PlanSIG 2006)},
  year={2006}
}

@inproceedings{conf/icml/WilsonFRT07,
  added-at = {2007-10-22T00:00:00.000+0200},
  author = {Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
  biburl = {https://www.bibsonomy.org/bibtex/26c270b99b2131b814972900a519097ca/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2007},
  editor = {Ghahramani, Zoubin},
  ee = {http://doi.acm.org/10.1145/1273496.1273624},
  interhash = {c0e007b9f1ae0ffcbdff982a6c6e37db},
  intrahash = {6c270b99b2131b814972900a519097ca},
  isbn = {978-1-59593-793-3},
  keywords = {dblp},
  pages = {1015-1022},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  timestamp = {2011-07-02T11:40:16.000+0200},
  title = {Multi-task reinforcement learning: a hierarchical Bayesian approach.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2007.html#WilsonFRT07},
  volume = 227,
  year = 2007
}

@article{DBLP:journals/corr/ParisottoBS15,
author = {Parisotto, Emilio and Ba, Lei Jimmy and Salakhutdinov, Ruslan},
file = {:Users/arnomoonens/Dropbox/MA2-AI/Thesis/papers/1511.06342.pdf:pdf},
journal = {CoRR},
mendeley-groups = {Thesis},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.06342},
volume = {abs/1511.0},
year = {2015}
}

@inproceedings{conf/icml/Bou-AmmarERT14,
  added-at = {2014-11-07T00:00:00.000+0100},
  author = {Bou-Ammar, Haitham and Eaton, Eric and Ruvolo, Paul and Taylor, Matthew E.},
  biburl = {https://www.bibsonomy.org/bibtex/228c5d610d6dce69411fa586adb157589/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2014},
  ee = {http://jmlr.org/proceedings/papers/v32/ammar14.html},
  interhash = {f8bab49ec43992c4025ad54d110fbdd7},
  intrahash = {28c5d610d6dce69411fa586adb157589},
  keywords = {dblp},
  pages = {1206-1214},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2016-07-13T11:44:02.000+0200},
  title = {Online Multi-Task Learning for Policy Gradient Methods.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2014.html#Bou-AmmarERT14},
  volume = 32,
  year = 2014
}

@article{Jaderberg2016Reinforcement,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the- art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87{\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03044v2},
author = {{Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki Tom Schaul, Joel Z Leibo}, David Silver {\&} Koray Kavukcuoglu},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1509.03044v2},
file = {:Users/arnomoonens/Dropbox/MA2-AI/Thesis/papers/1611.05397.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
mendeley-groups = {Thesis},
pages = {1--11},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
year = {2016}
}

@article{fernandez2013learning,
  added-at = {2013-03-14T00:00:00.000+0100},
  author = {Fern{\'a}ndez, Fernando and Veloso, Manuela M.},
  biburl = {https://www.bibsonomy.org/bibtex/20b95005c4f840ed0b161553b2f84db85/dblp},
  ee = {http://dx.doi.org/10.1007/s13748-012-0026-6},
  interhash = {96025d809004222c62a4a78e2eb49408},
  intrahash = {0b95005c4f840ed0b161553b2f84db85},
  journal = {Progress in AI},
  keywords = {dblp},
  number = 1,
  pages = {13-27},
  timestamp = {2013-08-13T14:26:20.000+0200},
  title = {Learning domain structure through probabilistic policy reuse in reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/journals/pai/pai2.html#FernandezV13},
  volume = 2,
  year = 2013
}

@inproceedings{Deisenroth2014Multi,
  added-at = {2014-12-16T00:00:00.000+0100},
  author = {Deisenroth, Marc Peter and Englert, Peter and Peters, Jan and Fox, Dieter},
  biburl = {https://www.bibsonomy.org/bibtex/2f314ce70a126ca341edad3a4cb4fa968/dblp},
  booktitle = {ICRA},
  crossref = {conf/icra/2014},
  ee = {http://dx.doi.org/10.1109/ICRA.2014.6907421},
  interhash = {b0350b1eb0fa8cf2058cdc2ec513c7e4},
  intrahash = {f314ce70a126ca341edad3a4cb4fa968},
  keywords = {dblp},
  pages = {3876-3881},
  publisher = {IEEE},
  timestamp = {2015-04-28T06:09:44.000+0200},
  title = {Multi-task policy search for robotics.},
  url = {http://dblp.uni-trier.de/db/conf/icra/icra2014.html#DeisenrothEPF14},
  year = 2014
}

@article{Ammar2014,
abstract = {Transfer learning can improve the reinforcement learning of a new task by allowing the agent to reuse knowledge acquired from other source tasks. Despite their success, transfer learning methods rely on having relevant source tasks; transfer from inappropriate tasks can inhibit performance on the new task. For fully automonomous transfer, it is critial to have a method for automatically choosing relevant source tasks, which requires a similarity measure between Markov Decision Processes (MDPs). This issue has received little attention, and is therefore still a largely open problem. This paper presents a data-driven automated similarity measure for MDPs. This novel measure is a significant step toward autonomous reinforcement learning transfer, allowing agents to: (1) characterize when transfer will be useful and, (2) automatically select tasks to use for transfer. The proposed measure is based on the reconstruction error of a restricted Boltzmann machine that attempts to model the behavioral dynamics of the two MDPs being compared. Empirical results illustrate that this measure is correlated with the performance of transfer and therefore can be used to identify similar source tasks for transfer learning.},
author = {Ammar, Haitham Bou and Eaton, Eric},
file = {:Users/arnomoonens/Dropbox/MA2-AI/Thesis/papers/2014mlis-bouammar.pdf:pdf},
isbn = {9781577356684},
journal = {Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence},
keywords = {AAAI Technical Report WS-14-07},
mendeley-groups = {Thesis/Transfer learning},
pages = {31--37},
title = {{An Automated Measure of MDP Similarity for Transfer in Reinforcement Learning}},
url = {http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/viewPaper/8824},
year = {2014}
}

@inproceedings{conf/icml/LeeGRN09,
  added-at = {2014-04-08T12:12:41.000+0200},
  author = {Lee, Honglak and Grosse, Roger B. and Ranganath, Rajesh and Ng, Andrew Y.},
  biburl = {https://www.bibsonomy.org/bibtex/280fce42a8f3032c7675bb7eb3596f9b3/prlz77},
  booktitle = {ICML},
  crossref = {conf/icml/2009},
  editor = {Danyluk, Andrea Pohoreckyj and Bottou, Léon and Littman, Michael L.},
  ee = {http://doi.acm.org/10.1145/1553374.1553453},
  interhash = {0a2ef2a6d7c19dfede7813ba0cf2bc3c},
  intrahash = {80fce42a8f3032c7675bb7eb3596f9b3},
  isbn = {978-1-60558-516-1},
  keywords = {Convolutional belief deep for hierarchical learning networks of representations. scalable unsupervised},
  pages = 77,
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  timestamp = {2014-04-08T12:12:41.000+0200},
  title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2009.html#LeeGRN09},
  volume = 382,
  year = 2009
}

@article{journals/corr/Graves13,
  added-at = {2013-12-10T00:00:00.000+0100},
  author = {Graves, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2d3697b979a78e10841dcc7eaa1998466/dblp},
  ee = {http://arxiv.org/abs/1308.0850},
  interhash = {ec0258df377a752ce87a6f7c59dea06d},
  intrahash = {d3697b979a78e10841dcc7eaa1998466},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2013-12-11T11:35:04.000+0100},
  title = {Generating Sequences With Recurrent Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1308.html#Graves13},
  volume = {abs/1308.0850},
  year = 2013
}

@article{journals/corr/ChungGCB14,
  added-at = {2015-01-01T00:00:00.000+0100},
  author = {Chung, Junyoung and G\"{u}l\c{c}ehre, \c{C}aglar and Cho, KyungHyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/2c5857f4c77ff7f8ba21f3e1add3dfbfe/dblp},
  ee = {http://arxiv.org/abs/1412.3555},
  interhash = {02ea54d9eb79421bba8eff3372c944b8},
  intrahash = {c5857f4c77ff7f8ba21f3e1add3dfbfe},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2015-06-18T04:27:40.000+0200},
  title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#ChungGCB14},
  volume = {abs/1412.3555},
  year = 2014
}

@incollection{Smolensky1986,
 author = {Smolensky, P.},
 chapter = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {194--281},
 numpages = {88},
 url = {http://dl.acm.org/citation.cfm?id=104279.104290},
 acmid = {104290},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{grant1990modelling,
  title={Modelling cognitive aspects of complex control tasks},
  author={Grant, Simon},
  booktitle={Proceedings of the IFIP TC13 Third Interational Conference on Human-Computer Interaction},
  pages={1017--1018},
  year={1990},
  organization={North-Holland Publishing Co.}
}
\section{Related work}
% Types of knowledge that is transferred: Value functions, features extracted from value functions, heuristics, policies
% Different representation types: Graphs, relational representation, tables

% Representation transfer
    % options

% separation in subtasks

% Relational RL

% transferring instances
% transferring advice/preferences
Transfer learning methods for reinforcement learning can first be divided into 2 groups: methods that use task mappings and methods that do not. We will only discuss methods that do not use task mappings as our approach also does not use task mappings.\\

In one of the earliest works of transfer learning used for reinforcement learning \citep{conf/ijcai/SelfridgeSB85}, the transition function was gradually changed as to make the task harder. A cart pole task was used where the pole was long and light at first, but was made shorter and heavier over time. The total time spent learning on the sequence of gradually modified tasks was shorter than when trying to solve the hardest task directly.\\

Later work was generally focused on transferring from one source task to a target task, while in recent research, most of the times a more general scenario with possibly multiple sources is considered. The approaches that we will further discuss mostly focus on transferring from multiple sources instead of just one. We do this because our approach also allows the use of multiple source tasks. Furthermore, the  discussed algorithms that can use multiple source tasks generally also allow just one source task from which to transfer knowledge.\\

One approach, presented in \cite{lazaric2008transfer}, is to transfer transitions (also called \textit{instances} or \textit{samples}) gathered from the source tasks to the target task. However, transitions may not be useful if the target tasks differs too much from the source tasks. To solve this, transitions from source tasks are chosen based on the similarity of these tasks to the target task. After having trained on the source tasks and collected transitions, we also collect a few transitions from the target task. The similarity between a source task and the target task is then the probability that the transitions of the target task were generated by an approximated model of the source task.

\cite{Ammar2014} also focus on the problem of selecting the appropriate source tasks from which to transfer knowledge. Here, however, a Restricted Boltzmann Machine \citep{Smolensky1986} is used in order to generate a model of a source tasks that yields features that are relevant for characterizing the task. The model, generated using transitions of the source task, then tries to reconstruct samples of the target task. The similarity of a source and target task can then be assessed using the difference of the reconstructed transitions and the real transitions of the target task. Note however that the state and action spaces of the source tasks and target task needs to be the same.\\

Another possibility is to transfer a representation. In this case, characteristics are inferred from multiple source tasks.
In \cite{Bernstein99reusingold}, policies are averaged and can be applied for $n$ time steps on all states. This combination of policy, time steps to execute and states is called an option. The reasoning is that actions that are used a lot in a state in source tasks may also be useful in the target task. In \cite{perkins1999using}, these options are provided on beforehand. The agent then learns a single action-value function over these options using all the source tasks. These options along with the action-value function can then be transferred and used for the target task.\\

Instead of using a single policy or action-value function, one can also collect a library of policies and select one to use probabilistically, depending on the expected reward. This approach is presented in \cite{fernandez2006probabilistic,fernandez2013learning}. At every time step, the algorithm can choose to use one of the source task policies, use the current best target task policy or to randomly explore. As the probabilities depend on the gained rewards, after some time more useful policies are exploited more often. This method only works however for tasks where only the goal state in a maze is different.\\

Unlike \cite{conf/cira/TanakaY03}, action-value functions are transferred instead of policies and statistics about them are used. More specifically, the average and standard deviation of the value each state-action pair is calculated. In the target task, the action-value function for each state-action pair is then instantiated to the average for that pair for the source tasks. The standard deviation is used in order to prioritize updates for state-action pairs that are far from the average. Besides this, states-action pairs that fluctuate often within an episode are also prioritized.\\

\cite{journals/ml/FosterD02} try to extract sub-tasks across multiple source tasks. Optimal policies are then learned for each of these sub-tasks and pieced together. In this case, the environment was a maze and tasks differ in their goal in the maze. Value functions can then be learned on parts of the maze. Less learning for the target task is then required because most of the already learned sub-tasks can also be used for the target task.\\

Most methods focus on problems with a discrete state and action space. However, other methods exist that can be applied on problems with a continuous state and action space. Here, function approximation is always required. \cite{walsh2006transferring} group states encountered in the source tasks and treat them as being one and the same state. This abstraction along with the value function learned on this abstraction can then be transferred and used for learning the target task.\\
\cite{lazaric2008knowledge} also groups states of the source tasks, but does this by adjusting parameters of a function approximator to build features. Only a small set of features are searched while still being able to learn useful value functions. The learning process for the tasks is executed in parallel. Again, after learning the source tasks, the parameters of the function approximator and the value functions are transferred.

2 other approaches use a hierarchical Bayesian model. They use this to find parameters that make up the dynamics and reward function of a problem. In \cite{sunmola2006model}, transitions of source tasks are used as a prior to find parameters for models. After getting transitions from the target task, the most probable model for the target task transitions is transferred and used.\\
\cite{conf/icml/WilsonFRT07} use a similar approach, but they don't make a distinction between source and target tasks. Instead, problems (MDPs) are executed sequentially and models are built using an already acquired set of transitions and models of previous tasks.\\

Together with the interest in deep reinforcement learning \citep{Mnih2015Human-levelLearning,Mnih2016AsynchronousLearning}, the interest in its application to transfer learning also grows. In \cite{DBLP:journals/corr/ParisottoBS15}, the algorithm, Actor-Mimic Network (AMN), learns the $Q$ function for source tasks from an expert for each specific task (in this paper a DQN that was trained until convergence).
Note that only one set of parameters (and thus one neural network) is used to learn on all the source tasks. As can be seen, this is supervised learning (more specifically regression) with the output of the expert's network as target value. As input, both the expert and the AMN can be used to sample trajectories.
Besides mimicking using the expert's output, the AMN also tries to mimic the hidden layer activations of the expert's network. This is done by adding a loss in function of the difference between the hidden layer activations of the expert network and the activations of the AMN. Intuitively, this gives insight to the AMN, also called to student, why it should act in a specific way, in addition to telling how it should act.
After learning for a pre-determined number of frames on each task, the weights of the AMN are transferred to a DQN and the algorithm trains on an unseen target task.
Note that each task is a different game here (i.e. the state space, action space, transition function and reward function can be different). However, the algorithm can also be applied to tasks with the same state and action space.

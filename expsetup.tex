\chapter{Experimental setup}
Experiments were executed on variations of the \textit{cart-pole} environment and on variations of the \textit{acrobot} environment.
For executing algorithms on both environments, a framework by \cite{Brockman2016OpenAIGym} was used.\\
After describing these environments, we describe the methodology of our experiments and the results.\\

\section{Cart-pole environment} % (fold)
\label{sub:cartpole_environment}
In the 2-dimensional \textit{cart-pole} environment \parencite{journals/tsmc/BartoSA83}, a pole is placed vertically on a cart.
The goal in this environment is to keep the pole balanced vertically (i.e.\ keep the angle of the pole between thresholds) and to keep the cart between 2 borders.
An agent can either move left or right. It can't stay at its current position.
The state is defined by 4 continuous-valued attributes: the position of the cart, the velocity of the cart, the angle of the pole and the angular velocity of the pole.
A discrete value of $1$ is given as a reward each time the pole is balanced and the cart is between bounds.
The episode ends either when these requirements are not fulfilled anymore or 200 steps have been executed.
The environment is visualized in Figure~\ref{fig:cartpole}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.6\linewidth]{images/cartpole.png}
    \caption[Visualization of the \emph{cart-pole} environment]{Visualization of the \emph{cart-pole} environment. $F$ is the force applied to the cart when taking an action. $x$ is the distance of the cart from the center and $\dot{x}$ is its velocity. $\theta$ is the angle of the pole and $\dot{\theta}$ refers to its angular velocity. Source: \cite{grant1990modelling}.}
    \label{fig:cartpole}
\end{figure}
% section cartpole_environment (end)

\section{Acrobot environment} % (fold)
\label{sub:acrobot_environment}
The goal in the \textit{acrobot} environment, visualized in Figure~\ref{fig:acrobot}, is to swing up the tip of 2 joined arms above a certain point \parencite{spong1995swing}.
This can be done by applying a force on the joint between the 2 arms.
However, this force is not enough to fulfill the goal immediately. Instead, the actuator must apply force to the left and to the right to build up enough speed to get above the horizontal threshold.
The state consists of the angle and angular velocity of both arms. One can either move the joint clockwise, counter-clockwise or do nothing.
An episode stops either when the tip of the outermost arm is above the threshold or when 500 steps have been executed in the episode.
At each step, a reward of $-1$ is given.
The goal is to maximize the reward and as such to minimize the amount of steps necessary to reach the threshold.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.7\linewidth]{images/acrobot.png}
    \caption[Visualization of the \emph{acrobot} environment]{Visualization of the \emph{acrobot} environment. $\theta_1$ and $\dot{\theta_1}$ are respectively the angle and angular velocity of the arm attached to the central fixed point. $\theta_2$ and $\dot{\theta_2}$ are respectively the angle and angular velocity of the outermost arm. Source: \cite{fremaux2013reinforcement}.}
    \label{fig:acrobot}
\end{figure}
% section acrobot_environment (end)

\section{Methodology} % (fold)
\label{sub:methodology}
Several experiments were executed with our transfer learning algorithm, using varying types of transferred knowledge and artificial neural networks and a different number of source tasks. However, the structure of the algorithm is always the same.\\
In some experiments, the performance of our algorithm is also compared to the performance of the \textit{REINFORCE} algorithm described in Section~\ref{sub:rl:policy_gradient}. When necessary, changes to this algorithm are mentioned.\\
The used values for hyperparameters for both algorithms are described in Appendix~\ref{sec:artificial_neural_network_parameters}.\\

For an experiment with our algorithm, first a number of environments are randomly generated. We try to learn with either 5 or 10 environments and thus source tasks.
The environment for each task can differ for a predefined number of parameters specific to the task, of which the values can each be in a certain range. For example for a \emph{cart-pole} task, these are the mass of the cart, the mass of the pole and the length of the pole.
The parameters and the allowed ranges for their values are described for each environment in Appendix~\ref{sec:environment_parameters}.\\
Then a number of source tasks are learned that can update both the shared knowledge base and their own sparse representation.
After a number of epochs, i.e.\ updates to these variables, we stop with learning these tasks.
Instead, we learn to solve a new task, the target task.
In some experiments, the sparse representation for the target task is initialized using the sparse representation of a randomly chosen source task.
In others, the sparse representation is randomly initialized, as explained in Appendix~\ref{sec:artificial_neural_network_parameters}.
However, for the target task only the sparse representation can be updated and not the shared knowledge base.\\

Other experiments involve the \textit{REINFORCE} algorithm and was used as comparison with the results of the other experiments.
Here, we first run the algorithm on one source task and then transfer the learned knowledge to the target task.
In practice, this means that we start learning on the target task with the weights learned using the source task.
The algorithm can still learn adapt all the weights using experience gained on the target task.
No sparseness on the weights was enforced.\\

Each experiment was repeated 100 times, each time using a different set of environments and thus problems to solve.
Afterwards the rewards are averaged over all the runs.\\
Trajectories for both sets of tasks contained of maximally 200 or 500 steps depending on the type of environment.
This could be less in case the environment was in an end state.
In case of the \emph{cart-pole} environment, this can mean for example that the cart tried to cross the left or right border or that the pole fell down.\\
Hyperparameters, such as the learning rate of artificial neural networks, were chosen manually.
% section methodology (end)

\section{Results}
To evaluate our proposed algorithm, we execute multiple experiments. First we compare the sequential and parallel version of our algorithm. We then see if feature extraction can improve the performance. Afterwards, we discuss results of an experiment where we transfer a sparse representation from a random source task to the target task. Last, we evaluate the results of using a single source task.\\
When discussing the jumpstart and asymptotic performance, we use the mean reward of respectively the first 5 and the last 5 epochs (i.e.\ updates to the artificial neural network parameters).

\subsection{Parallel and sequential knowledge transfer} % (fold)
\label{sub:parallel_and_sequential_knowledge_transfer}
We first compare the performance of the algorithm that learns the source tasks in parallel with the one that learns them sequentially. We do this for the \emph{cart-pole} environment in order to determine which version of the algorithm to use for further experiments.

The performance of both versions of the algorithm and for both the source tasks and target task is shown in Figure~\ref{fig:CartPole:reward_akt-kt_all_5tasks}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/kt_akt/reward_source-target_5tasks.png}
    \caption[Learning curves for the parallel and sequential version of our algorithm applied to the \textit{cart-pole} environment]{Learning curves for the parallel and sequential version of our algorithm applied to the \textit{cart-pole} environment. Until epoch 100, the average performance on the source tasks is shown. Afterwards, the performance of the target task is shown.}
    \label{fig:CartPole:reward_akt-kt_all_5tasks}
\end{figure}
Here, we see that, on average, the parallel version of our algorithm converges faster than the sequential version.
To find out which version is better, we first compare the median of the area under curve (AUC) over all the runs.
For the sequential version, the AUC is $19446.096$, while it is $19655.268$ for the parallel version.
We now use a Wilcoxon rank-sum test with the null-hypothesis that there is no difference.
We get $W=8.244$ and $1.665*10^{-16}$.
With a significance level of $0.05$, we can reject the null-hypothesis and say that the medians of the 2 versions are different.
By observing those medians, we can say that the parallel version is better. As it does not first accumulate gradients but applies them as they are computed, it instantly affects other tasks and improves their performance.\\
For the following experiments, when referring to "our transfer learning algorithm", the parallel version will be used.
% subsection parallel_and_sequential_knowledge_transfer (end)

\subsection{Feature extraction} % (fold)
\label{sub:feature_extraction}
In this experiment, we evaluate the use of feature extraction on the states. We compare the performance of an artificial neural network with a layer of 5 units, one with 10 units and one without feature extraction at all. The learning curves for the performance on the target task are shown in Figure~\ref{fig:CartPole:feature_extraction}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/feature_extraction.png}
    \caption{Learning curves for the transfer learning algorithm applied to the \textit{cart-pole} environment with no feature extraction in its neural network, one using a layer with 5 units and one using a layer of 10 units.}
    \label{fig:CartPole:feature_extraction}
\end{figure}
Feature extraction does not seem to improve the performance of the algorithm for this environment. It only seems to slow down learning. It is worse when we use more units in the hidden layer used for feature extraction. As more units are added, more weights influence the result and more of these weights must be learned.\\
It must be noted however that for algorithms working with high-dimensional inputs, feature extraction, using for example deep learning, is necessary to obtain a good performance. In this environment, the state contains values that are useful for solving a task. However, without feature extraction, it is not clear if a certain pixel of an image is important. Feature extraction is able to obtain values relevant for solving the task.\\

For the following experiments, we will focus on the different algorithms and different ways of transferring knowledge instead of the artificial neural network architecture.
% subsection feature_extraction (end)

\subsection{Usage of a different amount of source tasks} % (fold)
\label{sub:without_sparse_representation_transfer}
We now explore the differences between learning directly on a target task and first learning on source tasks (i.e.\ using our transfer learning algorithm), using the metrics that we defined in Section~\ref{sub:tl_metrics}.
We consider both 5 and 10 source tasks.
The algorithms using 5 source tasks and 10 source tasks are referred to as respectively \textit{TLA 5} and \textit{TLA 10}.
For the regular algorithm, we use the \textit{REINFORCE} that was discussed in Section~\ref{sub:rl:policy_gradient}.

\subsubsection{Cart-pole} % (fold)
\label{ssub:without_sparse_representation_transfer:cartpole}
The learning curve of both algorithms on the target task is visualized in Figure~\ref{fig:CartPole:reward_target_re-akt5-akt10}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/no_sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for \textit{REINFORCE} and \textit{TLA} for the \emph{Cart-pole} environment]{Learning curves for the \textit{cart-pole} environment of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}). The learning curves are only shown until epoch 60 in order to better show the initial performance of the algorithms. All the algorithms converged and achieved the same rewards after this epoch.}
    \label{fig:CartPole:reward_target_re-akt5-akt10}
\end{figure}
It can be seen visually that, on average, in all the configurations the jumpstart performance is about the same. We now compare the means, standard deviations and the medians of the jumpstart performances of the algorithms. This is shown in Table~\ref{tab:cartpole:nosparse:stats}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & Mean & Standard deviation & Median \\
    \hline
       REINFORCE  & $86.060$ & $25.330$ & $86.886$ \\
       \textit{TLA 5} & $158.237$ & $23.153$ & $163.840$ \\
       \textit{TLA 10} & $\bm{165.120}$ & $21.955$ & $\bm{169.034}$ \\
    \hline
    \end{tabular}
    \caption{Mean, standard deviation and median of the jumpstart performances for \textit{REINFORCE}, \textit{TLA 5} and \textit{TLA 10} applied to the cart-pole environment.}
    \label{tab:cartpole:nosparse:stats}
\end{table}
It can be seen that the best jumpstart performance is achieved using \textit{TLA 10}, as it has the highest mean and median and varies the least. Because it learned on more source tasks, it has more samples from the distribution of environment parameters and thus it can cover more kinds of task variations. \textit{REINFORCE} performs the worst, as it must learn without any prior learned knowledge.\\

We can also see in Figure~\ref{fig:CartPole:reward_target_re-akt5-akt10} that \textit{REINFORCE} takes longer on average to reach the maximum reward (which is $200$ in this environment). As a result, the area under curve for \textit{REINFORCE} is $18170.243$ whereas it is $19130.824$ and $19329.318$ for respectively \textit{TLA 5} and \textit{TLA 10}.\\
In every configuration, the maximum reward is reached most of the times as the median asymptotic performances for the 3 algorithms are all $200$.

\subsubsection{Acrobot} % (fold)
\label{ssub:without_sparse_representation_transfer:acrobot}
We now do with the same experiment, but with the \textit{acrobot} environment.
As can be seen in Figure~\ref{fig:Acrobot:reward_target_re-akt5-akt10}, the average performance of the \textit{REINFORCE} algorithm on the target task is different from \textit{TLA 5} and \textit{TLA 10}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/no_sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for the \textit{acrobot} environment of \textit{REINFORCE} and \textit{TLA} for the \emph{Acrobot} environment]{Learning curves of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}).}
    \label{fig:Acrobot:reward_target_re-akt5-akt10}
\end{figure}

We can how exactly the behavior of these algorithms differ initially by looking at the jumpstart performances. Statistics of these jumpstart performances are shown in Table~\ref{tab:acrobot:nosparse:stats}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & Mean & Standard deviation & Median \\
    \hline
       \textit{REINFORCE}  & $-361.209$ & $\bm{159.760}$ & $-490.063$ \\
       \textit{TLA 5} & $-380.222$ & $160.589$ & $-500.000$ \\
       \textit{TLA 10} & $\bm{-339.530}$ & $168.845$ & $\bm{-419.571}$ \\
    \hline
    \end{tabular}
    \caption{Mean, standard deviation and median of the jumpstart performances for \textit{REINFORCE}, \textit{TLA 5} and \textit{TLA 10} applied to the \textit{acrobot} environment.}
    \label{tab:acrobot:nosparse:stats}
\end{table}
We see that \textit{TLA 10} performs the best. However, as seen by the standard deviations, the results can differ considerably. We will see if the differences in medians are significant by using a Wilcoxon rank-sum test with for each pair of algorithms the null-hypothesis that there is no difference between them.
The resulting p-values are shown in Table~\ref{tab:sparse:pvalues}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & \textit{REINFORCE} & \textit{TLA 5} & \textit{TLA 10} \\
    \hline
       \textit{REINFORCE}  & & & \\
       \textit{TLA 5} & $0.512$ & & \\
       \textit{TLA 10} & $0.426$ & $0.179$ & \\
    \hline
    \end{tabular}
    \caption{P-values for the Wilcoxon rank-sum test of the jumpstart performances using different algorithms, applied to the \textit{acrobot} environment.}
    \label{tab:sparse:pvalues}
\end{table}
We can say using each time a significance level of $0.05$ that there is no difference between any combination of algorithms.\\

As we could already see in Figure~\ref{fig:Acrobot:reward_target_re-akt5-akt10}, the \textit{REINFORCE} algorithm seems learn slower on average than the other algorithms.
This is also visible when looking at the area under curve, which is $-30810.360$ for \textit{REINFORCE} algorithm and $-19437.733$ and $-16223.803$ for respectively \textit{TLA 5} and \textit{TLA 10}.\\

Last, we compare the asymptotic performances, again with the same test and the null-hypotheses that there are no differences between the asymptotic performances. The resulting p-values are shown in Table~\ref{tab:nosparse:asymp:pvalues}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & \textit{REINFORCE} & \textit{TLA 5} & \textit{TLA 10} \\
    \hline
       \textit{REINFORCE}  & & & \\
       \textit{TLA 5} & $1.034*10^{-4}$ & & \\
       \textit{TLA 10} & $7.091*10^{-6}$ & $0.449$ & \\
    \hline
    \end{tabular}
    \caption{P-values for the Wilcoxon rank-sum test using different algorithms with the \textit{acrobot} environment.}
    \label{tab:nosparse:asymp:pvalues}
\end{table}

With a significance level of $0.05$, we can say that there is a difference between \textit{REINFORCE} and \textit{TLA 5} and between \textit{REINFORCE} and \textit{TLA 10}.
With the same significance level, we retain the null-hypothesis for \textit{TLA 5} and \textit{TLA 10}.
Thus, we can say that using knowledge transfer yields a significant difference in the resulting performance, when applied to the \textit{acrobot} environment.
To further discuss these differences, we look at the boxplots of the asymptotic performances for the three configurations. These are shown in Figure~\ref{fig:nosparse:Acrobot:asymp_target_re-akt5-akt10}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/no_sparse_transfer/asymp_target_re-akt5-akt10.png}
    \caption{Boxplots of the asymptotic performances of \textit{REINFORCE}, \textit{TLA 5} and \textit{TLA 10} using the \textit{acrobot} environment.}
    \label{fig:nosparse:Acrobot:asymp_target_re-akt5-akt10}
\end{figure}
We can see that the median for the \textit{REINFORCE} algorithm is still the lowest possible reward for this environment. This means that in at least half of the cases, the \textit{REINFORCE} algorithm cannot improve and cannot get past the minimum reward. In contrast, the medians of \textit{TLA 5} and \textit{TLA 10} are respectively $-105.305$ and $-98.391$. Only some outliers of these algorithms don't get past the minimum reward.
% subsubsection acrobot (end)

% To further explore the jumpstart performance, we will look at boxplots of the jumpstarts of both algorithms, computed using all the 100 runs of both algorithms. These are shown in Figure~\ref{fig:boxplot_tla_re_5envs}.
% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.8\linewidth]{images/results/CartPole/no_sparse_transfer/jumpstart_target_re-akt5-akt10.png}
%     \caption{Boxplot of the jumpstart performance of \textit{REINFORCE} and our transfer learning algorithm (\textit{TLA}).}
%     \label{fig:boxplot_tla_re_5envs}
% \end{figure}
% We can see that, although the mean jumpstart performances are different, there is little difference between the medians: $22.774$ for \textit{REINFORCE} and $23.459$ for our algorithm. However, our algorithm is able to reach a higher initial performance more often. In 75\% of the cases, the jumpstart performance for \textit{REINFORCE} is below $55.535$, while it is $160.595$ for our algorithm.

% subsubsection cartpole:without_sparse_representation_transfer (end)
\subsection{Transfer of sparse representation} % (fold)
\label{sub:with_sparse_representation_transfer}
We now explore the results of transferring a sparse representation.
After learning on the source tasks, we initialize the sparse representation weights of the target task with the sparse representation of a randomly chosen source task.
The algorithm can then change this sparse representation to improve its performance on the target task.
We compare the results to those when not transferring the sparse representation, both for the cart-pole and the \textit{acrobot} environment.
\subsubsection{Cart-pole} % (fold)
\label{ssub:with_sparse_representation_transfer:cartpole}
To see how both version of the algorithm differ on average in terms of learning performance, we take a look at the learning curves, which are shown in Figure~\ref{fig:CartPole:st:reward_target_without-with}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/sparse_transfer/reward_target_without_with.png}
    \caption[Learning curves for the \textit{cart-pole} environment of \textit{TLA} with and without sparse representation transfer]{Learning curves for the \emph{Cart-pole} environment of our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}), both with and without sparse representation transfer.}
    \label{fig:CartPole:st:reward_target_without-with}
\end{figure}
It can be seen that the versions using sparse representation transfer need little to no update to reach the maximum reward. However, all versions converge to the maximum as all the medians of the asymptotic performances are $200$.

We now compare the jumpstart performances of these algorithms using the mean, standard deviation and median, shown in Table~\ref{tab:cartpole:sparse:jumpstart:stats}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & Sparse repr. transfer & Mean & Standard deviation & Median \\
    \hline
       \textit{TLA 5} & No & $158.237$ & $23.153$ & $163.840$ \\
       \textit{TLA 10} & No & $165.120$ & $21.955$ & $169.034$ \\
       \textit{TLA 5} & Yes & $192.613$ & $28.179$ & $\bm{200.000}$ \\
       \textit{TLA 10} & Yes & $\bm{196.125}$ & $21.640$ & $\bm{200.000}$ \\
    \hline
    \end{tabular}
    \caption{Mean, standard deviation and median of the jumpstart performances for \textit{TLA 5} and \textit{TLA 10}, with without and with sparse representation transfer, applied to the cart-pole environment.}
    \label{tab:cartpole:sparse:jumpstart:stats}
\end{table}
As shown, the versions using sparse representation transfer perform better initially. In at least half the runs, the maximum reward is already reached at the start. Just like for the version without sparse representation transfer, in most runs the maximum is maintained or reached at the end.

% We now determine if the jumpstart performances differ significantly, again using a Wilcoxon rank-sum test with the null-hypothesis that there is no difference. The resulting p-values are shown in
% subsubsection cartpole (end)
\subsubsection{Acrobot} % (fold)
\label{ssub:with_sparse_representation_transfer:acrobot}
To discuss the differences between the 2 versions for the \textit{acrobot} environment, we also look at the learning curves, which are shown in Figure~\ref{fig:Acrobot:st:reward_target_without-with}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/sparse_transfer/reward_target_without_with.png}
    \caption[Learning curves for the \textit{acrobot} environment of \textit{TLA} with and without sparse representation transfer]{Learning curves for the \emph{acrobot} environment of our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}), both with and without sparse representation transfer.}
    \label{fig:Acrobot:st:reward_target_without-with}
\end{figure}
It can be seen that, on average, the version using sparse representation transfer can start with a high reward but improves little afterwards.
However, the rewards stay superior to those using the version without sparse representation transfer.\\

First, we see how the 2 versions perform initially, using the jumpstart performances. Statistics about the jumpstart performances are presented in Table~\ref{tab:acrobot:sparse:stats}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & Sparse repr. transfer & Mean & Standard deviation & Median \\
    \hline
       \textit{TLA 5} & No & $-380.222$ & $160.589$ & $-500.000$ \\
       \textit{TLA 10} & No & $-339.530$ & $168.845$ & $-419.571$ \\
       \textit{TLA 5} & Yes & $-141.236$ & $101.567$ & $-110.482$ \\
       \textit{TLA 10} & Yes & $\bm{-136.400}$ & $97.188$ & $\bm{-108.157}$ \\
    \hline
    \end{tabular}
    \caption{Mean, standard deviation and median of the jumpstart performances \textit{TLA 5} and \textit{TLA 10} both with and without sparse representation transfer, applied to the \textit{acrobot} environment.}
    \label{tab:acrobot:sparse:stats}
\end{table}
Indeed, the versions using sparse representation transfer initially obtain a higher reward and are more stable. This means that the sparse representation of a source task is a better initialization than just using random values.\\
Next, we use the same types of statistics for the asymptotic performances.
These are shown in Table~\ref{tab:acrobot:sparse:asymp:stats}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & Sparse repr. transfer & Mean & Standard deviation & Median \\
    \hline
       \textit{TLA 5} & No & $-156.128$ & $137.661$ & $-107.222$ \\
       \textit{TLA 10} & No & $-132.873$ & $105.777$ & $-96.224$ \\
       \textit{TLA 5} & Yes & $-110.481$ & $67.384$ & $-93.085$ \\
       \textit{TLA 10} & Yes & $\bm{-106.626}$ & $64.108$ & $\bm{-91.087}$ \\
    \hline
    \end{tabular}
    \caption{Mean, standard deviation and median of the asymptotic performances \textit{TLA 5} and \textit{TLA 10} both with and without sparse representation transfer, applied to the \textit{acrobot} environment.}
    \label{tab:acrobot:sparse:asymp:stats}
\end{table}
Again, the versions with sparse representation transfer perform better, because a higher mean and median reward is achieved and the standard deviations are lower.\\
Using these statistics, we see that for this environment a sparse representation is also a useful and more stable starting point for the target task and also leads to a better performance at the end.
We can also notice that the performance is better when using using 10 source tasks instead of 5. To see if this difference is significant, we use a Wilcoxon rank-sum test with the null-hypothesis that there is no difference in asymptotic performances. This results in a p-value of $0.654$. Thus, with a significance level of $0.05$, we can retain the null-hypothesis and say that the difference is not significant.
% subsubsection acrobot (end)
% subsection with_sparse_representation_transfer (end)

\subsection{REINFORCE using a source and target task} % (fold)
\label{sub:reinforce_using_a_source_and_target_task}
To see if it is really necessary to learn from multiple source tasks that use a shared knowledge base, we apply the \textit{REINFORCE} algorithm first on one source task and then use the learned weights of the artificial neural network as initialization for the network of the target task. Our transfer learning algorithm is used for comparison and first learns on 5 source tasks. It then transfers besides the shared knowledge base also a randomly chosen sparse representation from one of the source tasks, like in Section~\ref{sub:with_sparse_representation_transfer}.\\
We execute this experiment both for the \textit{cart-pole} and the \textit{acrobot} task.

\subsubsection{Cart-pole} % (fold)
\label{ssub:reinforce_source_target:cart_pole}
For the \textit{cart-pole} environment, the learning curves are shown in Figure~\ref{fig:carpole:reward_reinforce_2tasks}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/reinforce_2tasks.png}
    \caption{\textit{REINFORCE} applied for 100 epochs to a randomly chosen source task of a \textit{cart-pole} environment and afterwards to the target task, using the same network and weight values. The learning curve is compared to the one of the \textit{TLA 5} algorithm that uses sparse representation transfer.}
    \label{fig:carpole:reward_reinforce_2tasks}
\end{figure}
% subsubsection cart_pole (end)
It can be seen that on average the \textit{TLA 5} algorithm learns more quickly on the source tasks. It is also able to retain a better jumpstart performance, with a median of $200$ instead of $186.908$ for \textit{REINFORCE}. The asymptotic performances seem to be equal and have the same median of $200$.

\subsubsection{Acrobot} % (fold)
\label{ssub:reinforce_source_target:acrobot}
We now do the same experiment for the \textit{acrobot} environment. The resulting learning curve for the source and target task is shown in Figure~\ref{fig:acrobot:reward_reinforce_2tasks}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/reinforce_2tasks.png}
    \caption{\textit{REINFORCE} applied for 100 epochs to a randomly chosen source task of an \textit{acrobot} and afterwards to the target task, using the same network and weight values.}
    \label{fig:acrobot:reward_reinforce_2tasks}
\end{figure}
On average, for both algorithms only a small adaption is needed for the target task to reach the same reward as the one with which the source task ended. However, the median jumpstart performance is $-500$, meaning that in at least half the cases the knowledge of the source task does not improve the initial performance on the target task. In contrast, the median jumpstart performance for \textit{TLA 5} is $-110.482$. For the target task, the asymptotic performance of \textit{REINFORCE} also does not reach the values from our \textit{TLA 5} algorithm. The \textit{REINFORCE} algorithm has a median asymptotic performance of $-203.426$ instead of $-93.085$ for \textit{TLA 5}.
% subsection reinforce_using_a_source_and_target_task (end)
% subsection acrobot (end)


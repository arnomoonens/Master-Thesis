\chapter{Experimental setup}
Experiments were executed on variations of the \textit{Cart-pole} environment and on variations of the \textit{Acrobot} environment.\\

\section{CartPole environment} % (fold)
\label{sub:cartpole_environment}
In the 2-dimensional Cart-Pole environment, a pole is placed vertically on a cart. The goal in this environment is to keep the pole balanced vertically (i.e. keep the angle of the pole between thresholds) and to keep the cart between 2 borders.
The state is defined by 4 continuous-valued attributes: the position of the cart, the velocity of the cart, the angle of the pole and the angular velocity of the pole.
A discrete value of $1$ is given as a reward each time the pole is balanced and the cart is between bounds.
The episode ends either when these requirements are not fulfilled anymore or 200 steps have been executed.
An agent can either move left or right. It can't stay at its current position.
The environment is visualized in Figure~\ref{fig:cartpole}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.6\linewidth]{images/cartpole.png}
    \caption[Cart-Pole environment]{Cart-Pole environment. $F$ is the force applied to the cart when taking an action. $x$ is the distance of the cart from the center and $\dot{x}$ is its velocity. $\theta$ is the angle of the pole and $\dot{\theta}$ refers to its angular velocity. Source: \cite{grant1990modelling}.}
    \label{fig:cartpole}
\end{figure}
For executing the experiments, an implementation of this environment by \cite{Brockman2016OpenAIGym} was used.\\
% section cartpole_environment (end)

\section{Acrobot environment} % (fold)
\label{sub:acrobot_environment}
The goal in the \textit{Acrobot} environment, visualized in Figure~\ref{fig:acrobot} is to swing up  the tip of 2 joined arms above a certain point \cite{spong1995swing}.
This can be done by applying a force on the joint between the 2 arms.
However, this force is not enough to fulfill the goal immediately. Instead, the actuator must apply force to the left and to the right to build up enough speed to get above the horizontal threshold.
The state consists of the angle and angular velocity of both arms. One can either move the joint clockwise, counter-clockwise or do nothing.
An episode stops either when the tip of the outermost arm is above the threshold or when 500 steps have been executed in the episode.
The goal is to minimize the amount of steps necessary to reach the threshold.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.7\linewidth]{images/acrobot.png}
    \caption[Acrobot environment]{Acrobot environment. $\theta_1$ and $\dot{\theta_1}$ are respectively the angle and angular velocity of the arm attached to the central fixed point. $\theta_2$ and $\dot{\theta_2}$ are respectively the angle and angular velocity of the outermost arm. Source: \cite{fremaux2013reinforcement}}
    \label{fig:acrobot}
\end{figure}
% section acrobot_environment (end)

\section{Methodology} % (fold)
\label{sub:methodology}
Several experiments were executed, depending on the algorithm, the environment, the number of source tasks and which knowledge was transferred to the target task.\\

In the first experiment, first a number of environments are randomly generated. We try to learn with both 5 and 10 environments.
The environment for each task can differ for a predefined number of attributes, of which the values can each be in a certain range. For a cart-pole task, these are the mass of the cart, the mass of the pole and the length of the pole.\\
Then a number of source tasks are learned that can update both the shared knowledge base and their own sparse representation. After a number of epochs, i.e. updates to these variables, we stop with learning these tasks.
Instead, we learn to solve a new task, the target task, for which we hadn't learned its sparse representation yet. Here however, only the sparse representation can be updated and not the shared knowledge base.\\

In the second experiment, besides transferring the shared knowledge base, a sparse representation was also transferred.
This is a sparse representation randomly chosen from the source tasks.
The target task can then change this sparse representation to improve the performance on its own problem instance.\\

The last experiment involves the \textit{REINFORCE} algorithm and was used as comparison with the results of the other experiments.
Here, we first run the algorithm on one source task then transfer the same knowledge to the target task.
In practice, this means that we start learning on the target task with the weights learned using the source task.
The algorithm can still learn adapt all the weights using the target task.
No sparseness on the weights was enforced.\\

All three experiments were executed on the cart-pole and acrobot environment.
Each experiment consisted of 100 runs, each time using a different set of environments and thus problems to solve.\\
Afterwards the rewards are averaged over all the runs.\\
Trajectories for both sets of tasks contained of maximally 200 or 500 steps depending on the type of environment. This could be less in case the environment was in an end state. In case of the cart-pole environment, this can mean for example that the cart tried to cross the left or right border or that the pole fell down.\\
Hyperparameters, such as the learning rate of neural networks, were chosen by iteratively executing the previously mentioned experiment. Possible values for each hyperparameter were chosen manually.
% section methodology (end)

\section{Results}
Here we compare the results of our transfer learning algorithm with those of the \textit{REINFORCE} algorithm.
\subsection{CartPole} % (fold)
\label{sub:cartpole}
\subsubsection{Without sparse representation transfer} % (fold)
\label{ssub:cartpole:without_sparse_representation_transfer}
We first explore the differences between learning directly on a target task and first learning on source tasks (i.e. using our transfer learning algorithm), using the metrics that we defined in Section~\ref{sub:tl_metrics}. We consider both 5 and 10 source tasks. The algorithms using 5 source tasks and 10 source tasks are referred to as respectively \textit{TLA 5} and \textit{TLA 10}.
For the regular algorithm, we use the \textit{REINFORCE} that was discussed in Section~\ref{sub:rl_policy_gradient}. The learning curve of both algorithms on the target task is visualized in Figure~\ref{fig:CartPole:reward_target_re-akt5-akt10}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/no_sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for \textit{REINFORCE} and \textit{TLA} for the \emph{CartPole} environment]{Learning curves for the \textit{CartPole} environment of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}). The learning curves are only shown until epoch 60 in order to better show the initial performance of the algorithms. All the algorithms converged and achieved the same rewards after this epoch.}
    \label{fig:CartPole:reward_target_re-akt5-akt10}
\end{figure}
It can be seen visually that, on average, in all the configurations the jumpstart performance is about the same. This is confirmed by using the Wilcoxon signed-rank test. The null-hypothesis is that there is no difference between the jumpstart performance of 2 algorithms.
\todo{Complete table}
The p-values for all combinations are shown in Table~\ref{tab:nosparse:pvalues}:
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & \textit{REINFORCE} & \textit{TLA 5} & \textit{TLA 10} \\
    \hline
       \textit{REINFORCE}  & NA & & \\
       \textit{TLA 5} & $0.413$ & NA & \\
       \textit{TLA 10} & $0.097$ & $0.415$ & NA \\
    \hline
    \end{tabular}
    \caption{P-values for the Wilcoxon signed-rank test using different algorithms.}
    \label{tab:nosparse:pvalues}
\end{table}

With a significance level of $0.05$, we can say that the null-hypotheses should be retained and there is no difference between the jumpstart performances of any of the algorithms.\\
However, we can see again in Figure~\ref{fig:CartPole:reward_target_re-akt5-akt10} that \textit{REINFORCE} takes a lot longer on average to reach the maximum reward (which is $200$ in this environment). As a result, the area under curve for \textit{REINFORCE} is $18170.243$ whereas it is $19603.523$ and $19329.318$ for respectively \textit{TLA 5} and \textit{TLA 10}.\\
In every configuration, the maximum reward ($200$) is reached eventually and as such they have the same asymptotic performance.\\

% To further explore the jumpstart performance, we will look at boxplots of the jumpstarts of both algorithms, computed using all the 100 runs of both algorithms. These are shown in Figure~\ref{fig:boxplot_tla_re_5envs}.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.8\linewidth]{images/results/CartPole/no_sparse_transfer/jumpstart_target_re-akt5-akt10.png}
%     \caption{Boxplot of the jumpstart performance of \textit{REINFORCE} and our transfer learning algorithm (\textit{TLA}).}
%     \label{fig:boxplot_tla_re_5envs}
% \end{figure}
% We can see that, although the mean jumpstart performances are different, there is little difference between the medians: $22.774$ for \textit{REINFORCE} and $23.459$ for our algorithm. However, our algorithm is able to reach a higher initial performance more often. In 75\% of the cases, the jumpstart performance for \textit{REINFORCE} is below $55.535$, while it is $160.595$ for our algorithm.

% subsubsection cartpole:without_sparse_representation_transfer (end)
\subsubsection{With sparse representation transfer} % (fold)
\label{ssub:cartpole:with_sparse_representation_transfer}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/CartPole/sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for the \textit{CartPole} environment of \textit{REINFORCE} and \textit{TLA} using sparse representation transfer]{Learning curves for the \emph{CartPole} environment of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}). The sparse representation of a randomly chosen source task was transferred to the target task.}
    \label{fig:CartPole:st:reward_target_re-akt5-akt10}
\end{figure}
% subsubsection with_sparse_representation_transfer (end)
% subsection cartpole (end)

\subsection{Acrobot} % (fold)
\label{sub:acrobot}
\subsubsection{Without sparse representation transfer} % (fold)
\label{ssub:acrobot:without_sparse_representation_transfer}
We will start with the same experiment as in Section~\ref{ssub:cartpole:without_sparse_representation_transfer}, but with the \textit{Acrobot} environment.
As can be seen in Figure~\ref{fig:Acrobot:reward_target_re-akt5-akt10}, the average performance of  the \textit{REINFORCE} algorithm on the target task is different from \textit{TLA 5} and \textit{TLA 10}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/no_sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for the \textit{Acrobot} environment of \textit{REINFORCE} and \textit{TLA} for the \emph{Acrobot} environment]{Learning curves of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}).}
    \label{fig:Acrobot:reward_target_re-akt5-akt10}
\end{figure}

We will now discuss how exactly the behavior of these algorithms differ.\\
We start by looking at the jumpstart performances. Again, we use the Wilcoxon signed-rank test with the null-hypothesis that there is no difference between the initial performances of the algorithms. The resulting p-values are shown in Table~\ref{tab:sparse:pvalues}.
\begin{table}[htb]
    \centering
    \begin{tabular}{llccc}
    \hline
    Algorithm & \textit{REINFORCE} & \textit{TLA 5} & \textit{TLA 10} \\
    \hline
       \textit{REINFORCE}  & NA & & \\
       \textit{TLA 5} & $0.0047$ & NA & \\
       \textit{TLA 10} & $0.8143$ & $0.0032$ & NA \\
    \hline
    \end{tabular}
    \caption{P-values for the Wilcoxon signed-rank test using different algorithms where the \textit{TLA} algorithms use sparse representation transfer.}
    \label{tab:sparse:pvalues}
\end{table}
We see that, with a significance level of $0.05$, the jumpstart performance of \textit{TLA 5} is different from the jumpstart performance of the other configurations. To discuss, this further, we look at boxplots of the jumpstart performances of all algorithms, over all their runs. These are shown in Figure~\ref{fig:Acrobot:jumpstart_target_re-akt5-akt10}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{images/results/Acrobot/no_sparse_transfer/jumpstart_target_re-akt5-akt10.png}
    \caption{Boxplots of the jumpstart performances of \textit{REINFORCE}, \textit{TLA 5} and \textit{TLA 10} using the \textit{Acrobot} environment.}
    \label{fig:Acrobot:jumpstart_target_re-akt5-akt10}
\end{figure}
Indeed, although the medians in all configurations are $-500$ (the lowest reward possible for this environment), the initial performances seem to be spread more for \textit{REINFORCE} and \textit{TLA 10} than for \textit{TLA 5}.\\

As we could already see in Figure~\ref{fig:Acrobot:reward_target_re-akt5-akt10}, the performance of the \textit{REINFORCE} algorithms seems to be lower on average than the other algorithms. This is also visible when looking at the area under curve, which is $-30810.360$ for \textit{REINFORCE} algorithm and $-19437.733$ and $-16223.803$ for respectively \textit{TLA 5} and \textit{TLA 10}.\\

Last, we compare the asymptotic performances, again with the same test and the null-hypotheses that there are no differences between the asymptotic performances. We get the following p-values:
\begin{itemize}
    \item \textit{REINFORCE} and \textit{TLA 5}: $0.0000$
    \item \textit{REINFORCE} and \textit{TLA 10}: $0.0000$
    \item \textit{TLA 5} and \textit{TLA 10}: $0.2902$
\end{itemize}
With a significance level of $0.05$, we can say that there is a difference between \textit{REINFORCE} and \textit{TLA 5} and between \textit{REINFORCE} and \textit{TLA 10}. With the same significance level, we retain the null-hypothesis for \textit{TLA 5} and \textit{TLA 10}. To further discuss these differences, we again look at the boxplots of the asymptotic performances for the three configurations. These are shown in Figure~\ref{fig:Acrobot:asymp_target_re-akt5-akt10}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{images/results/Acrobot/no_sparse_transfer/asymp_target_re-akt5-akt10.png}
    \caption{Boxplots of the asymptotic performances of \textit{REINFORCE}, \textit{TLA 5} and \textit{TLA 10} using the \textit{Acrobot} environment.}
    \label{fig:Acrobot:asymp_target_re-akt5-akt10}
\end{figure}
We can see that the median for the \textit{REINFORCE} algorithm is still the lowest possible reward for this environment. The medians of \textit{TLA 5} and \textit{TLA 10} are respectively $-105.3053$ and $-98.3911$. Only some outliers of these algorithms don't get past the minimum reward.

% subsubsection acrobot:without_sparse_representation_transfer (end)
\subsubsection{With sparse representation transfer} % (fold)
\label{ssub:acrobot:with_sparse_representation_transfer}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/sparse_transfer/reward_target_re-akt5-akt10.png}
    \caption[Learning curves for the \textit{Acrobot} environment of \textit{REINFORCE} and \textit{TLA} using sparse representation transfer]{Learning curves for the \emph{Acrobot} environment of the \textit{REINFORCE} algorithm and our transfer learning algorithm (\textit{TLA}) with 5 source tasks (shown as \textit{TLA 5}) and 10 source tasks (shown as \textit{TLA 10}). The sparse representation of a randomly chosen source task was transferred to the target task.}
    \label{fig:Acrobot:st:reward_target_re-akt5-akt10}
\end{figure}
% subsubsection acrobot:with_sparse_representation_transfer (end)
\subsubsection{REINFORCE using a source and target task} % (fold)
\label{ssub:reinforce_using_a_source_and_target_task}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{images/results/Acrobot/reinforce_2tasks.png}
    \caption{\textit{REINFORCE} applied for 100 epochs to a randomly chosen source task and afterwards to the target task, using the same network and weight values.}
    \label{fig:Acrobot:reward_reinforce_2tasks}
\end{figure}
% subsubsection reinforce_using_a_source_and_target_task (end)
% subsection acrobot (end)

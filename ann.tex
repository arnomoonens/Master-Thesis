\section{Artificial neural networks} % (fold)
\label{sec:artificial_neural_networks}
% What is it
Artificial neural networks are models that can approximate discrete-valued, real-valued and vector-valued functions.
They are composed of simple units that can be activated by other units and can further activate multiple other units using connections of varying strength.
%citation?
Artificial neural networks are loosely inspired by biological neural networks, where these units are called neurons, which are connected by axons.\\
The higher the strength, also called weight, between units, the more influence the unit has on the connected one. These strengths can be manually defined. However, thanks to a method called backpropagation, these weights can be learned systematically.
The combination of artificial neural networks and backpropagation led to successful applications, for example to recognize handwritten characters \citep{journals/neco/LeCunBDHHHJ89}, for face recognition \citep{cottrell1990extracting} and to recognize spoken words \citep{journals/nn/LangWH90}.
However, they were also used for reinforcement learning, for example to learn to balance a pendulum \citep{anderson:ieeecsm89} or play Backgammon \citep{Tesauro:92}.

\subsection{Basics} % (fold)
\label{sub:basics}
As said, artificial neural networks are made up of units. These units can be divided into layer, where the values of each layer are propagated to the next layer depending on connections with weights. These weights define the model. A simple artificial neural network is visualized in Figure~\ref{fig:ann}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.7\linewidth]{images/ann.png}
    \caption[An artificial neural network]{An artificial neural network with 3 input units, 4 hidden units and 2 output units. $w_{ih}$ and $w_{ho}$ are the weights for connections between respectively the input units and hidden units and between the hidden units and output units.}
    \label{fig:ann}
\end{figure}
Input units receive information that is available externally and provide information to other units. Output units are the opposite and receive information from the network itself and provide information externally. Hidden units both receive information from and provide information to units inside the network.\\
As a non-input unit gets weighted activations from multiple units, these need to be combined to determine if the unit can be activated or not. This is done by applying a linear or non-linear function to the weighted sum.
% subsection basics (end)

\subsection{Activation functions} % (fold)
\label{sub:activation_functions}
Formally, an activation function $\phi$ will compute for unit $j$ with inputs from layer $x$:
\begin{equation}
    o = \phi\big(\sum_i^n w_{ij} x_i\big)
\end{equation}
Or in vector format:
\begin{equation}
    o = \phi(\overrightarrow{w} \cdot \overrightarrow{x})
\end{equation}
Here, we will discuss the most commonly used activation functions.
\subsubsection{Perceptron} % (fold)
\label{ssub:perceptron}
A perceptron, defined by \cite{Rosenblatt58}, gives as output either $-1$ or $+1$:
\begin{equation}
e_t(s) = \begin{cases}
+1 & \text{if $\sum_i^n w_{ij} x_i > 0$}\\
-1 & \text{otherwise}
\end{cases}
\end{equation}
It can be seen as a hyperplane where the output is $-1$ or $+1$ depending on which side the input lies.
Using $-1$ as \texttt{false} and $+1$ as \texttt{true}, it is also possible to represent boolean functions such as \textit{AND}, \textit{OR}, \textit{NAND} and \textit{NOR}. However, some boolean functions, such as \textit{XOR} cannot be represented by a single perceptron \citep{ML}.

\todo{fix citation}
A schematic using the perceptron function is shown in Figure~\ref{fig:perceptron}.\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{images/activation_functions/perceptron.png}
    \caption[Perceptron function applied to the inputs for a single unit]{Perceptron function applied to the inputs for a single unit.Source: \cite{Taspinar2016}.}
    \label{fig:perceptron}
\end{figure}

For the definitions of the following activation functions, we will assume that $x$ is already the weighted sum of the input units.
% subsubsection perceptron (end)

\subsubsection{Sigmoid} % (fold)
\label{ssub:sigmoid}
A disadvantage of a perceptron is that it is not differentiable in the whole domain (specifically at $x=0$). Why this is a problem will become clear in Section~\ref{sub:gradient_descent_and_backpropagation}.\\
To solve this problem, a sigmoid function is used. A sigmoid function is a differentiable function that is monotonically increasing approaches an asymptote for $x \to \pm\infty$ \citep{series/lncs/LeCunBOM12}. As a result, these can still separate the input space in 2 parts.\\
In the artificial neural networks domain, the \textit{sigmoid function} generally refers to a variation of the logistic function and is denoted by $\sigma(x)$. Its formula is:
\begin{align}
    \begin{split}
        f(x) &= \sigma(x)\\
        &= \frac{1}{1+e^{-x}}
    \end{split}
\end{align}
The behavior of this function is visualized in Figure~\ref{fig:sigmoid}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/activation_functions/sigmoid.png}
    \caption[The sigmoid function]{The sigmoid function.}
    \label{fig:sigmoid}
\end{figure}
% subsubsection sigmoid (end)

\subsubsection{Hyperbolic tangent} % (fold)
\label{ssub:hyperbolic_tangent}
Another popular function and another kind of sigmoid function is the hyperbolic tangent function, also called \textit{tanh}. While the output of the sigmoid function ranges between $0$ and $1$, here the output ranges between $-1$ and $+1$, just like the 2 possible values of the perceptron.
The formula of the hyperbolic tangent function, which can also be written in terms of the sigmoid function, is:
\begin{align}
    \begin{split}
        f(x) &= tanh(x)\\
        &= 2\sigma(2x) - 1\\
        &= \frac{e^{\frac{x}{2}} - e^{-\frac{x}{2}}}{e^{\frac{x}{2}} + e^{-\frac{x}{2}}}
    \end{split}
\end{align}
This function is visualized in Figure~\ref{fig:tanh}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/activation_functions/tanh.png}
    \caption{Hyperbolic tangent function.}
    \label{fig:tanh}
\end{figure}
% subsubsection hyperbolic_tangent (end)

\subsubsection{Rectified Linear Unit} % (fold)
\label{ssub:rectified_linear_unit}
A rectified linear unit, also called a ReLU, is another popular and more recent activation function. It was first defined by \cite{conf/icml/NairH10}. The output is the identity function if $x \ge 0$ and $0$ otherwise:
\begin{equation}
    f(x) = \begin{cases}
        x & \text{if $x \ge 0$}\\
        0 & \text{otherwise}
        \end{cases}
\end{equation}
As can be seen, this function requires less computations than the sigmoid and hyperbolic tangent function. However, the derivative for $x<0$ is always $0$. Why this can be a problem is explained in Section~\ref{sub:gradient_descent_and_backpropagation}.\\
The ReLU function is shown in Figure~\ref{fig:relu}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/activation_functions/relu.png}
    \caption[Rectified Linear Unit function]{Rectified Linear Unit function.}
    \label{fig:relu}
\end{figure}

To solve the problem of the derivative being zero at $x<0$, the Leaky ReLU was invented \citep{maas2013rectifier}. Here, instead of the output being zero for $x<0$, the output has a small slope, defined by a constant $\alpha$:
\begin{equation}
    f(x) = \begin{cases}
        x & \text{if $x \ge 0$}\\
        \alpha x & \text{otherwise}
        \end{cases}
\end{equation}
As can be seen, the derivative of this function is always non-zero.\\
An example of a Leaky ReLU is shown in Figure~\ref{fig:lrelu}.\\
\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{images/activation_functions/lrelu.png}
    \caption[Leaky ReLU function]{Leaky ReLU function with $\alpha = 0.2$.}
    \label{fig:lrelu}
\end{figure}
% subsubsection rectified_linear_unit (end)
% subsection activation_functions (end)

A variation of a Leaky ReLU, called Parametric Rectified Unit, also has a small slope for $x<0$, but with an $\alpha$ that can be learned \citep{journals/corr/HeZR015}. Other variants use a random slope for values below zero \citep{journals/corr/XuWCL15}, add noise a ReLU \citep{conf/icml/NairH10} or use an exponential function for values below zero \citep{journals/corr/ClevertUH15}:
\begin{equation}
    f(x) = \begin{cases}
        x & \text{if $x \ge 0$}\\
        \alpha (e^x-1) & \text{otherwise}
        \end{cases}
\end{equation}
%Optional: add softmax and maxout definitions

\subsection{Gradient descent and backpropagation} % (fold)
\label{sub:gradient_descent_and_backpropagation}
%Also something about linearly separable?
%Why no algebraic solution?
If we choose the weights of our network correctly, we might succeed in the approximating a function.
However, it is not always possible to separate the input space with our artificial neural network. In that case, we might want the best solution, e.g. one that has the least amount of errors in producing outputs w.r.t. the correct outputs, also called the target outputs. Furthermore, choosing the weights manually can be a tedious process.\\
For these reasons, we use an algorithm called called backpropagation, which uses a variation of gradient descent. We will first discuss gradient descent as this provides the basis of the backpropagation algorithm.

\subsubsection{Gradient descent} % (fold)
\label{ssub:gradient_descent}
In gradient descent, we search in the weight vector space, also called hypothesis space, to find weights that can approximate a function as best as possible. The tuple of the input and output of the target function is called a training example.

To be able to update weights, we first need to know how wrong our outputs were relative to those of our training examples.
This is called the training error and depends on the learning algorithm (in our case an artificial neural network), its parameters and the training examples. However, here we assume that the learning algorithm and the training examples are fixed while learning.
A common measure for the training error is:
\begin{equation}
    E(\overrightarrow{w}) \equiv \frac{1}{2} \sum_{i=0}^N (y_i - f(x_i;\overrightarrow{w}))^2
\end{equation}
Where $N$ is the number of training examples, $x_i$ is the input of a training example, $y_i$ is the target output of the training example and $f$ is our learning algorithm, which depends on the weight vector $\overrightarrow{w}$.

Because we now know how the training error and the weight vector is related, we can compute the derivative of the training error $E$ with respect to each component of $\overrightarrow{w}$. This called the gradient and is denoted as $\nabla E(\overrightarrow{w})$:
\begin{equation}
    \nabla E(\overrightarrow{w}) \equiv \left [ \frac{\partial E}{\partial w_0}, \frac{\partial E}{\partial w_1}, \dots, \frac{\partial E}{\partial w_n} \right ]
\end{equation}
Where $n$ is the number of components in $\overrightarrow{w}$. Note $\nabla E(\overrightarrow{w})$ is also a vector. It defines how to change the weight components in order to get the steepest increase in the training error $E$. Thus, if we negate the gradient, we get the steepest decrease in $E$. Thus, we can update the weight vector as such:
\begin{equation}
    \overrightarrow{w} \gets \overrightarrow{w} - \eta \nabla E(\overrightarrow{w})
\end{equation}
Where $\eta$ is a positive value called the learning rate and influences how big the changes to the weight vector are.
\todo{Define (local) minimum. Also add figure?}
A small value leads to slow convergence, while a high value can cause the algorithm to overstep a local minimum. Because of this, the some algorithms gradually decrease the learning rate as the number of weight updates grows.\\
For a single weight component, we get:
\begin{equation}
    \label{eq:weightcomponentupdate}
    w_i \gets w_i - \eta \frac{\delta E}{\delta w_i}
\end{equation}

Of course, in order to update the weights, we first need to calculate the derivate of $E$ w.r.t. each component $w_i$. As an example, we show the derivative when using a simple linear unit, which defined as:
\begin{equation}
    f(\overrightarrow{x};\overrightarrow{w}) = \overrightarrow{x} \cdot \overrightarrow{w}
\end{equation}
We then get the following derivation:
\begin{align}
    \label{eq:gradderiv}
    \begin{split}
        \frac{\delta E}{\delta w_i} &= \frac{\delta}{\delta w_i} \frac{1}{2} \sum_{j=0}^N (y_j - o_j)^2\\
        &= \frac{1}{2} \sum_{j=0}^N \frac{\delta}{\delta w_i} (y_j - o_j)^2\\
        &= \frac{1}{2} \sum_{j=0}^N 2 (y_j - o_j) \frac{\delta}{\delta w_i} (y_j - o_j)\\
        &= \sum_{j=0}^N (y_j - o_j) \frac{\delta}{\delta w_i} (y_i - \overrightarrow{x_i} \cdot \overrightarrow{w})\\
        &= \sum_{j=0}^N (y_j - o_j)(-x_{ij})
    \end{split}
\end{align}
Where $x_{ij}$ is the $i$'th input component of training example $j$ and $o_j \equiv f(x_j; \overrightarrow{w})$. Instead of just using a linear unit, the function $f$ can of course be more complex and use for example on of the activation functions from Section~\ref{sub:activation_functions}. However, we can now see that we cannot use the perceptron because it is not differentiable.\\

Using the resulting formula of Equation~\ref{eq:gradderiv} along with Equation~\ref{eq:weightcomponentupdate}, we can also define how to update each component of the weight vector:
\begin{equation}
    w_i \gets w_i + \eta \sum_{j=0}^N (y_j - o_j)x_{ij}
\end{equation}
As we can see, for each weight vector, we need to apply the model to the input of each training example. For this reason, this version of gradient descent is also often called \textit{batch gradient descent}. Of course, the outputs only need to be computed once per weight vector update as they do not depend on which specific weight vector component that we are updating.\\

Still, each time using every training example for the weight vector update can lead to slow convergence to a local minimum. Furthermore, in case of multiple local minima in the error surface, it is possible that the gradient descent algorithm does not stop at the global minimum \citep{ML}.
% subsubsection gradient_descent (end)

\subsubsection{Stochastic gradient descent} % (fold)
\label{ssub:stochastic_gradient_descent}
A popular variation of batch gradient descent that tries to solve the previously mentioned issues is \textit{stochastic gradient descent}. Instead of summing using all training examples, we do a weight update using only one training example. To do this, we first use a different error function for each example $j$:
\begin{equation}
    E_j(\overrightarrow{w}) = \frac{1}{2}(y_j - o_j)^2
\end{equation}
The update for a single weight vector component when using a linear unit is then:
\begin{equation}
    w_i \gets w_i + \eta (y_j- o_j)x_{ij}
\end{equation}
The idea here is that these weight updates, when having iterated over all the training examples, will be a decent approximation relative to using our original error function. Note that the update using one training example affects the error of the next training example.\\

By making the learning rate small enough, usually smaller than with batch gradient descent, it is possible to approximate true gradient descent arbitrarily closely \citep{ML}.
It is also computationally cheaper because each time we only handle one training example.
Additionally, stochastic gradient descent can sometimes avoid being stuck in local minima because it uses various $\nabla E_j(\overrightarrow{w})$ instead of just $\nabla E(\overrightarrow{w})$ to move in the hypothesis space.
% subsubsection stochastic_gradient_descent (end)

\subsubsection{Backpropagation} % (fold)
\label{ssub:backpropagation}
The backpropagation algorithm uses gradient descent and can learn weights of a multilayer network with possibly multiple units in each layer \citep{rumelhart1986learning}.

We say that, for multilayer networks, $L>2$, with $L$ the number of layers in the network. This means that there are other (hidden) layers besides the input and output layer. Such a network was already depicted in Figure~\ref{fig:ann}.

As multiple output units are also possible, we need a new training error measure that sums over all of the output units of the network:
\begin{equation}
    E(\overrightarrow{w}) \equiv \frac{1}{2} \sum_{i=0}^N \sum_{k \in outputs} (y_{ki} - o_{ki})^2
\end{equation}
Where $outputs$ are the output units of the network and $y_{ki}$ and $o_{ki}$ is the value of the $k$'th output unit of respectively the $i$'th training example and network output.\\

The difference with calculating the weight changes in artificial neural networks with more than 2 layers is that, starting from the second non-input layer, the input to a unit comes, besides the weights, from other units of which the value itself is also calculated.
Thus, units in some layers can depend on units in multiple previous layers. This must also be taken into account when calculating the gradient for the output of those units.
To do this, we can use the chain rule for the gradient of a single weight component:
\begin{align}
    \frac{\partial E_d}{\partial w_{ji}} &= \frac{\partial E_d}{\partial net_j} \frac{\partial net_j}{\partial w_{ji}}\\
    &= \frac{\partial E_d}{\partial net_j} x_{ji}
\end{align}
Where $E_d$ is the error for a training example $d$, $x_{ji}$ is the $i$'th input to unit $j$, $w_{ji}$ is the weight associated with this input and $net_j = \sum_i w_{ji} x_{ji}$. The further derivation for all the weights of the network is described in \cite[Chapter~4]{ML}. For an artificial neural network with 1 hidden layer, we get the pseudo-code shown in Algorithm~\ref{algo:backpropagation}.\\
\begin{algorithm}[htb]
\DontPrintSemicolon
\KwIn{training\_examples, neural\_network, $\eta$}
\emph{// Assume a network with $n_{in}$ input units $n_{hidden}$ hidden units and $n_{out}$ output units.}\\
\emph{// Assume (randomly) instantiated weights.}\\
\Repeat{a termination condition is met}{
    \For{$(\overrightarrow{x},\overrightarrow{y})$ in training\_examples}{
        Propagate $\overrightarrow{x}$ through the network and receive values for all the output units: $o_u$ with $u$ an output unit of the network\\
        \For{each network output unit $k$}{
            Calculate its error term:\\
            $\delta_k \gets o_k(1-o_k)(y_k-o_k)$
        }
        \For{each hidden unit $h$}{
            Calculate its error term:\\
            $\delta_h \gets o_h(1-o_h) \sum_{k \in outputs} w_{kh}\delta_k$
        }
        \For{each network weight $w_{ji}$}{
            Do an update:\\
            $w_{ji} \gets w_{ji} + \Delta w_{ji}$\\
            where\\
            $\Delta w_{ji} = \eta \delta_j x_{ji}$ \label{algo:backpropagation:deltaweight}
        }
    }
}
\caption[Backpropagation]{Backpropagation algorithm. Source: \cite[Chapter~4]{ML}.}
\label{algo:backpropagation}
\end{algorithm}
% subsubsection backpropagation (end)
\subsubsection{Extensions and improvements} % (fold)
\label{ssub:extensions_and_improvements}
One of the most common extensions to backpropagation is to add a fraction of the previous weight update to the new weight update. This extension is called \textit{momentum}.
\todo{add equation ref}
To apply this, we change the equation on line~\ref{algo:backpropagation:deltaweight} of Algorithm \ref{algo:backpropagation} to the following:
\begin{equation}
    \Delta w_{ji}(t) = \eta \delta_j x_{ji} + \alpha \Delta w_{ji}(t-1)
\end{equation}
Where $\Delta w_{ji}(t)$ is a weight update executed at iteration $t$ and $\alpha$, with $0 \le \alpha < 1$, determines how much of the previous weight update we want to carry to the new one. When $\alpha = 0$, we get again the regular weight update.

Thanks to momentum, it is possible to get past regions where the gradient is zero and the weights would not change without momentum.
This is analogous to a ball rolling down a surface but keeps rolling a bit on a flat surface afterwards. In the real world, an object in motion will stay in motion, unless there is a force applied to it.
Here, that force is the gradient. It is also possible to get past small local minima and it can speed up learning when the gradient does not change. Again, this is analogous to a ball that goes downhill and keeps gaining speed.
% subsubsection extensions_and_improvements (end)
% subsection gradient_descent_and_backpropagation (end)

% Inspired by biology

% section artificial_neural_networks (end)